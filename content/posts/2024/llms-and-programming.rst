llms and programming
####################
:date: 2024-08-13 11:07
:author: admin
:category: Uncategorized
:slug: llms-and-programming
:status: published
:save_as: 2024/08/13/llms-and-programming/index.html
:url: 2024/08/13/llms-and-programming/

Tom MacWright has written `an interesting post on LLMs and their effect on the discipline of programming <https://macwright.com/2024/07/18/llms-democratizing-coding>`__, noting that they represent a grimly ironic answer to his desire to democratize programming, since his enthusiasm for the project was about its potential to provide both intellectual and financial returns. LLMs allow programmers to write code without understanding it and to increase productivity without increasing skill, which might undermine compensation standards. Becoming a programmer is getting easier! But at the expense of the reasons for doing it at all.

I don't disagree with Tom's big-picture take. But I think there are a few more things worth considering, which I offer from a perspective that--I'm sorry to say--is a bit of a blind spot for him: that of a much worse programmer.

I am not abysmal, mind you. In particular, I write code with a pretty good mental model of its probable resource consumption, and have okay judgment about when to expend effort moving up or down the optimization-and-scalability ladder. I've worked with a lot of technologies. I've shipped code--not earth-shattering code, but production code nonetheless--to audiences in the hundreds of millions. And, because I've worked alongside or nearby some truly excellent programmers (such as Tom himself) I can still often teach an early-career programmer a thing or two.

But my limits are significant. For one thing, I'm pretty rusty. When I need to get something done I reach for Python and bash, neither of which are exactly fast-evolving ecosystems. I have never been immersed in the scene of open source foment--I can tell you what ``bun`` is (was?), but not whether you should use it. And, most damningly, I don't care enough to get *really* good. Or maybe that's wrong: my problem is that my interests go down, not up. I am intrigued by instruction sets, registers, caches, half-adders, NAND gates, and electron holes, predictive branching calculations and DEF CON talks about weird side channel attacks. I like to know that stuff exists, and to understand it deeply enough to appreciate how it's connected, if not exactly how it's implemented. If you instead turn your gaze upward, away from system buses and laser-pulsed droplets of tin, the machine falls away entirely and you find yourself wandering amid a linguistic forest of abstract syntax trees. The arbitrariness of representational systems becomes apparent. You lose interest in giving computers lists of *things to do* and start giving them *ways to be.* Sometimes a quasi-mystical revelation arrives and you become able to create a new way of running global telecommunications infrastructure or stymying cybercriminals. Usually, you just wind up writing another LISP.

I have never had the affinity or assiduousness for that sort of thing, though I know enough to admire those who do. I am content to just bang out interpreted code to do something neat, hoping that if any part of it is too slow, someone smart will have put a compiled solution online for me to use. That's how good I am and probably how good I ever will be.

This turns out to be about the right level of competence for using LLMs. And let me just say: wow.

The React monoculture hadn't fully taken over by the time programming ceased to be my day job. I got the idea. But I'd never put in the reps for the plumbing to feel intuitive. "Yes, I could figure this out," I told myself, "But I have three kids and a real job." But now I just say: I'd like to use vite and Tailwind. I don't want my map to flicker when I update state. Please do not make me remember what a forward ref is for longer than is absolutely necessary. And it all works.

Another time: I wanted to build a mobile app. Mobile dev is a deep discipline that places enormous constraints on the developer, both in how they do their work and what their work can achieve relative to a normal computer, making it simultaneously intimidating and deflating. It's tempting to use a simplifying framework in a more familiar language--maybe with cross-platform compatibility?--but the fields of computing history are absolutely littered with such projects' corpses, making the investment of effort highly suspect. But now I can tell an LLM I want a Flutter list view based on some JSON it's pulled and, one Apple Developer fee later, I have an app for my stupid thermostat system.

Even native development is suddenly achievable. I couldn't possibly justify learning Swift and its associated ecosystems without a serious iOS need--you expect me to believe a language that's borrowed Perlisms like ``$0`` is elegantly designed?!--but now I don't have to learn it to use it.

What I can't yet tell is how bad I could be while still benefiting from all this. I learned a lot about programming in the pre-LLM era, and that context lets me see when the robot is suggesting a dependency that's out of date, or an optimization that's premature, or a level of abstraction that's inappropriate, or an architecture that's going to bite me in the ass, or a response to an error message that's irrelevant. If I started my career today, I don't think I would need to learn those things, or at least wouldn't come to understand them in the same way, and I consequently wouldn't be able to spot the robot's missteps.

But that's only one part of programming. A lot of it is about pasting error messages into websites, and ChatGPT is a much better website for that than Google ever was.

Perhaps more importantly, the only time I've reached truly new levels of skill--not just knowledge, skill--it was because I was sitting next to a programmer better than me. When a junior programmer can find someone like that who's willing to indulge them, it's an immensely valuable opportunity. Now a close approximation can be rented for a few bucks a month.

This pattern of benefits seems to be consistent. `Mediocre performers are helped by LLMs more than top performers <https://mitsloan.mit.edu/ideas-made-to-matter/workers-less-experience-gain-most-generative-ai>`__. So far, LLMs are an assistive technology. I choose to view this optimistically. I know a lot about computers, and much of it was learned during thankless hours when I was stuck searching for a path forward, without a better programmer sitting next to me. But much of the material I learned during those interludes was trivia--if I'd known what I was looking for I would have found it right away!--and only some of it was worth retaining. I don't cherish those hours.

The time I spent with a guiding hand expanded my understanding at a vastly greater rate, and I'm excited by the prospect of my children having an infinitely patient tutor available--for programming or whatever else--during the inevitable hours when a human isn't available. It's true that the palette of abstractions and details they learn will be twisted by LLM capabilities into a shape different from the one I developed. But that, at least, has ever been the case in CS. In my youth we thought all programmers needed to understand memory allocation!

I can't discuss the economic dimension of LLMs' impact as thoroughly, because it seems to me to be much less knowable. Frankly, I don't understand why programming is still such a good job to have. It's extremely outsource-able. It is, fundamentally, about moving information around to unlock efficiencies. Markets are pretty good about finding and shaving down inefficiencies and laptops and broadband connections are not big capital expenses. How much work remains? We are decades into the ICT revolution. `Noah Smith thinks the tech industry might be approaching maturity <https://www.noahpinion.blog/p/some-thoughts-on-the-future-of-the>`__, in a process analogous to the build-out of the national freight rail network. I'm less certain than he is, in part because what counts as "tech" seems mostly to be a function of what adopting the term means for the user's financing prospects. But it does seem plausible.

It's also hard to know what counterfactuals could have or might be possible. How much have the FAANG monopolists bid up salaries as a bet on their future and/or against their competitors'? How much more work would be available if the platforms hadn't swallowed the web, homogenizing and sanitizing it into a handful of monolithic services? These are inputs to the equation that I can't properly estimate.

Ultimately the economic question boils down to familiar arguments (once thought resolved, recently reopened on appeal): comparisons to Luddites, revisionist accounts of the Luddites, midwit memes about the Luddites; promises of expanding pies and productivity growth and redistribution under neoliberalism and how mad we are about all of it, whatever it might be, exactly.

I don't completely trust anyone's intuitions on these matters, certainly including my own. But inventing tools that let us do more is usually good, I think, especially when what we're doing more of is benign enough to remain safely confined to a screen.

I hope I'm dead before chatbots become digital people who displace my descendants' labor. When that happens I think it's going to be a rough time for everybody. But the chatbots who can write fragments of code are already here. I'm pretty sure about that. And I think it'll be okay.
